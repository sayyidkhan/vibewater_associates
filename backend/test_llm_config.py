#!/usr/bin/env python3
"""
Test script to verify LLM configuration (Anthropic vs Bedrock)
"""
import os
from dotenv import load_dotenv

# Load .env file
load_dotenv()

print("="*80)
print("ğŸ” LLM CONFIGURATION CHECK")
print("="*80)

# Check Anthropic API Key
anthropic_key = os.getenv('ANTHROPIC_API_KEY', '').strip()
anthropic_model = os.getenv('ANTHROPIC_MODEL', 'claude-3-5-haiku-20241022').strip()
print("\nğŸ“‹ Anthropic API Configuration:")
if anthropic_key:
    key_preview = anthropic_key[:10] + "..." if len(anthropic_key) > 10 else anthropic_key
    print(f"  âœ… API Key: {key_preview}")
    print(f"  Length: {len(anthropic_key)} characters")
    print(f"  Model: {anthropic_model}")
else:
    print(f"  âŒ API Key not found or blank")

# Check Bedrock Configuration
aws_token = os.getenv('AWS_BEARER_TOKEN_BEDROCK', '').strip()
print("\nğŸ“‹ AWS Bedrock Token:")
if aws_token:
    token_preview = aws_token[:10] + "..." if len(aws_token) > 10 else aws_token
    print(f"  âœ… Found: {token_preview}")
else:
    print(f"  âŒ Not found or blank")

print(f"\nğŸ“‹ AWS Region: {os.getenv('AWS_REGION', 'us-east-1')}")
print(f"ğŸ“‹ Bedrock Model ID: {os.getenv('BEDROCK_MODEL_ID', 'us.anthropic.claude-3-5-haiku-20241022-v1:0')}")

# Determine which service will be used
print("\n" + "="*80)
print("ğŸ¯ SERVICE SELECTION")
print("="*80)
if anthropic_key:
    print("  âœ… Will use: Anthropic API")
else:
    print("  â„¹ï¸  Will use: AWS Bedrock (fallback)")

print("\n" + "="*80)
print("ğŸ’¡ NEXT STEPS")
print("="*80)
if anthropic_key:
    print("âœ… Anthropic API key is configured!")
    print("   Restart your server to use Anthropic API:")
    print("   uv run uvicorn app.main:app --reload")
else:
    print("â„¹ï¸  Using Bedrock fallback")
    print("   To use Anthropic API instead:")
    print("   1. Add ANTHROPIC_API_KEY=your_key to .env file")
    print("   2. Restart the server")

print("="*80)
